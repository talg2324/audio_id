{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tals_feat_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfv7Qr_gskpu"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from scipy import stats, signal\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.layers import Conv2D,Conv1D, Dropout, MaxPooling2D, Flatten, Dense, Input , concatenate, MaxPooling1D, BatchNormalization, AveragePooling2D,AveragePooling1D\r\n",
        "from keras.losses import categorical_crossentropy\r\n",
        "from keras.utils import plot_model\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras import metrics\r\n",
        "import os\r\n",
        "from google.colab import drive\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import Normalizer , StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO3J-BRZua7u"
      },
      "source": [
        "drive.mount('/content/gdrive')\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRSdgX8duqOU"
      },
      "source": [
        "# Yariv\r\n",
        "feat_space = './drive/MyDrive/audio_colab/Features/'\r\n",
        "\r\n",
        "# Tal\r\n",
        "#feat_space = './drive/MyDrive/Voice Recording Database/Features/'\r\n",
        "\r\n",
        "X = {}\r\n",
        "Y = {}\r\n",
        "\r\n",
        "class_no = 0\r\n",
        "stat_names =[\r\n",
        "        'spectral_centroid',\r\n",
        "        'spectral_bandwidth',\r\n",
        "        'rms',\r\n",
        "        'zero_crossing_rate']\r\n",
        "\r\n",
        "mel_orders = ['mfcc', 'del-mfcc', 'del-del-mfcc']\r\n",
        "\r\n",
        "for spect_path in os.listdir(feat_space):\r\n",
        "    data = {}\r\n",
        "    if '.npy' in spect_path and 'Copy' not in spect_path:\r\n",
        "      label, ind = spect_path.split('_')\r\n",
        "\r\n",
        "      obj = np.load(feat_space+spect_path, allow_pickle=True)\r\n",
        "      stats = obj[0,0]\r\n",
        "      data['STFT'] = obj[0,1]\r\n",
        "      mel = obj[0,2]  \r\n",
        "      data['Spect'] = obj[0,3]\r\n",
        "      data['dSdT'] = obj[0,4]\r\n",
        "      data['dS2dT2'] = obj[0,5]\r\n",
        "\r\n",
        "      for i in range(len(stat_names)):\r\n",
        "          data[stat_names[i]] = stats[i,:]\r\n",
        "\r\n",
        "      for i in range(len(mel_orders)):\r\n",
        "          data[mel_orders[i]] = mel[i, :, :]\r\n",
        "\r\n",
        "      if label not in Y.keys():\r\n",
        "          Y[label] = class_no\r\n",
        "          X[class_no] = []\r\n",
        "          class_no += 1\r\n",
        "\r\n",
        "      this_class = Y[label]\r\n",
        "      X[this_class].append(data)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRdSlJKgx0MB"
      },
      "source": [
        "def get_model_block(inp_shape, dims, p=0.5):\r\n",
        "\r\n",
        "    if dims=='image':\r\n",
        "\r\n",
        "      inp = Input(shape= (inp_shape[0], inp_shape[1], 1))\r\n",
        "      x = Conv2D(256, kernel_size=(3,3), activation='relu', input_shape=inp_shape)(inp)\r\n",
        "      x = AveragePooling2D(pool_size=(2,2))(x)\r\n",
        "      x = Dropout(p)(x)\r\n",
        "      x = Conv2D(128, kernel_size=(3,3), activation='relu')(x)\r\n",
        "      x = AveragePooling2D(pool_size=(2,2))(x)\r\n",
        "      x = Dropout(p)(x)\r\n",
        "      x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)\r\n",
        "      x = AveragePooling2D(pool_size=(2,2))(x)\r\n",
        "      x = Dropout(2*p)(x)\r\n",
        "      x = Conv2D(32, kernel_size=(3,3), activation='relu')(x)\r\n",
        "      x = AveragePooling2D(pool_size=(2,2))(x)\r\n",
        "      x = Dropout(3*p)(x)\r\n",
        "      \r\n",
        "  \r\n",
        "      connection_layer = Flatten()(x)\r\n",
        "\r\n",
        "    if dims=='coeffs':\r\n",
        "\r\n",
        "      inp = Input(shape= (inp_shape[0], inp_shape[1], 1))\r\n",
        "      x = Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=inp_shape)(inp)\r\n",
        "      x = MaxPooling2D(pool_size=(2,2))(x)\r\n",
        "      x = Dropout(p)(x)\r\n",
        "      x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)\r\n",
        "      x = MaxPooling2D(pool_size=(2,2))(x)\r\n",
        "      x = Dropout(2*p)(x)\r\n",
        "      x = Conv2D(32, kernel_size=(3,3), activation='relu')(x)\r\n",
        "      x = MaxPooling2D(pool_size=(2,2))(x)\r\n",
        "      x = Dropout(3*p)(x)\r\n",
        "      \r\n",
        "      connection_layer = Flatten()(x)\r\n",
        "\r\n",
        "    if dims=='stats':\r\n",
        "      inp = Input(shape = (inp_shape[0],1) )\r\n",
        "      x = Conv1D(128, kernel_size=3, activation='relu', input_shape=inp_shape)(inp)\r\n",
        "      x = MaxPooling1D(pool_size=2)(x)\r\n",
        "      x = Dropout(p)(x)\r\n",
        "      x = Conv1D(64, kernel_size=3, activation='relu')(x)\r\n",
        "      x = MaxPooling1D(pool_size=2)(x)\r\n",
        "      x = Dropout(2*p)(x)\r\n",
        "      x = Conv1D(32, kernel_size=3, activation='relu')(x)\r\n",
        "      x = MaxPooling1D(pool_size=2)(x)\r\n",
        "      x = Dropout(3*p)(x)\r\n",
        "\r\n",
        "      connection_layer = Flatten()(x)\r\n",
        "\r\n",
        "    m = Model(inputs = inp,outputs = connection_layer)\r\n",
        "\r\n",
        "    return m\r\n",
        "\r\n",
        "def model_output(models,num_class,lr=1e-3, p=0.5):\r\n",
        "\r\n",
        "    if len(models) > 1 :\r\n",
        "      outputs = concatenate([ m.output for m in models ])\r\n",
        "      inputs = [ m.input for m in models ] \r\n",
        "\r\n",
        "    else:\r\n",
        "      outputs = concatenate([models[0].output,models[0].output])\r\n",
        "      inputs = [models[0].input]\r\n",
        "\r\n",
        "    x = Dense(32, activation='relu')(outputs)\r\n",
        "    x = Dropout(p)(x)\r\n",
        "    output_layer = Dense(num_class, activation='softmax')(outputs)\r\n",
        "\r\n",
        "    opt = Adam(lr)\r\n",
        "    m = Model(inputs = inputs ,outputs = output_layer)\r\n",
        "    m.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\r\n",
        "    return m "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOyXCrdHFdli"
      },
      "source": [
        "def normalize_reshape(x_train, x_test, feats):\r\n",
        "  Xtrain = {}\r\n",
        "  Xtest = {}\r\n",
        "\r\n",
        "  for f in feats:\r\n",
        "    train_len = len(x_train)\r\n",
        "    test_len = len(x_test)\r\n",
        "    orig_shape = x_train[0][f].shape\r\n",
        "    dims = len(orig_shape)\r\n",
        "    norm = Normalizer()\r\n",
        "\r\n",
        "    if dims > 1:\r\n",
        "      h,w = orig_shape\r\n",
        "      X0 = np.array([x[f].flatten() for x in x_train])\r\n",
        "      Xf = np.array([x[f].flatten() for x in x_test])\r\n",
        "\r\n",
        "      X0 = norm.fit_transform(X0).reshape(train_len, h, w)\r\n",
        "      Xf = norm.transform(Xf).reshape(test_len, h, w)\r\n",
        "\r\n",
        "    else:\r\n",
        "      l = orig_shape[0]\r\n",
        "      X0 = np.array([x[f] for x in x_train])\r\n",
        "      Xf = np.array([x[f] for x in x_test])\r\n",
        "\r\n",
        "      X0 = norm.fit_transform(X0).reshape(train_len, l, 1)\r\n",
        "      Xf = norm.transform(Xf).reshape(test_len, l, 1)\r\n",
        "\r\n",
        "    Xtrain[f] = X0\r\n",
        "    Xtest[f] = Xf\r\n",
        "\r\n",
        "  return Xtrain, Xtest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsRK2a8q4-ao"
      },
      "source": [
        "def wrapper(_X_train,_Y_train2,dim,feats):\r\n",
        "\r\n",
        "  done=False\r\n",
        "  best_loss = 100\r\n",
        "  best_feats = []\r\n",
        "  counter = 0\r\n",
        "  lr=1e-3\r\n",
        "  batch_size=16\r\n",
        "  dropout_block= 0.2\r\n",
        "  dropout_out= 0.7\r\n",
        "  epochs=85\r\n",
        "  split = 0.7\r\n",
        "  v = 0\r\n",
        "\r\n",
        "  while not done:\r\n",
        "    losses = []\r\n",
        "    x_test_round, x_train_round, y_test_round, y_train_round = train_test_split(_X_train, _Y_train2, test_size=split, random_state=7)\r\n",
        "    x_train_round, x_test_round = normalize_reshape(x_train_round, x_test_round, feats)\r\n",
        "    m_list = []\r\n",
        "    x_train = []\r\n",
        "    x_test = []\r\n",
        "\r\n",
        "    if len(best_feats):\r\n",
        "      \r\n",
        "      for f in best_feats:\r\n",
        "        \r\n",
        "        x_train.append(x_train_round[f])\r\n",
        "        x_test.append(x_test_round[f])\r\n",
        "\r\n",
        "        m_list.append(get_model_block(x_train_round[f][0].shape, dims[f], p=dropout_block))\r\n",
        "      #print('best features: ',best_feats)\r\n",
        "      #print('best loss :',best_loss)\r\n",
        "\r\n",
        "    for f in feats:\r\n",
        "      if f not in best_feats:\r\n",
        "        #print('add '+f)\r\n",
        "        x_train_i = []\r\n",
        "        x_test_i = []\r\n",
        "\r\n",
        "        x_train_i.extend([block for block in x_train])\r\n",
        "        x_test_i.extend([block for block in x_test])\r\n",
        "\r\n",
        "        x_train_i.append(x_train_round[f])\r\n",
        "        x_test_i.append(x_test_round[f])\r\n",
        "\r\n",
        "        m_list.append(get_model_block(x_train_round[f][0].shape,dims[f], p=dropout_block))\r\n",
        "\r\n",
        "        m = model_output(m_list, num_class=Y_train.max()+1,lr=lr, p=dropout_out)\r\n",
        "\r\n",
        "        if len(m_list) > 1:\r\n",
        "          h = m.fit(x_train_i, y_train_round, epochs=epochs, batch_size=batch_size, verbose=v)\r\n",
        "          loss, acc = m.evaluate(x_test_i, y_test_round, batch_size=batch_size)\r\n",
        "        else:\r\n",
        "          h = m.fit(x_train_i, y_train_round, epochs=epochs, batch_size=batch_size, verbose=v)\r\n",
        "          loss, acc = m.evaluate(x_test_i, y_test_round, batch_size=batch_size)\r\n",
        "\r\n",
        "        #print('train acc:' + str(max(h.history['accuracy'])))\r\n",
        "\r\n",
        "        losses.append(loss)\r\n",
        "        m_list.pop()\r\n",
        "\r\n",
        "    best_round = min(losses)\r\n",
        "    best_feat_round = losses.index(best_round) \r\n",
        "    \r\n",
        "    if best_round < best_loss*0.99:\r\n",
        "      best_loss = best_round\r\n",
        "      best_feats.append(feats[best_feat_round+counter])\r\n",
        "      hyper_params = [lr,batch_size,dropout_block,dropout_out,epochs,split] \r\n",
        "      counter+=1\r\n",
        "      \r\n",
        "    else:\r\n",
        "      break\r\n",
        "\r\n",
        "    if len(best_feats) == 3:\r\n",
        "      print('best feats: ', best_feats)\r\n",
        "      print(hyper_params)\r\n",
        "      break\r\n",
        "\r\n",
        "    else:\r\n",
        "      r=np.random.uniform(0.0001,0.001)\r\n",
        "      batch_size= np.random.randint(16,28)\r\n",
        "      dropout_block= np.random.uniform(0.1,0.3)\r\n",
        "      dropout_out= np.random.uniform(0.5,0.7)\r\n",
        "      epochs=np.random.randint(40,80)\r\n",
        "      split = np.random.uniform(0.7,0.75)\r\n",
        "      \r\n",
        "  return sorted(best_feats, key=str.lower), hyper_params, best_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG4_9e1ZC1SO"
      },
      "source": [
        "def extract_best_feat(_best_feats,best_params,best_loss):\r\n",
        "\r\n",
        "  freq_dict = {}\r\n",
        "  losses_best_feat = []\r\n",
        "  min_key_loss = 100 \r\n",
        "\r\n",
        "  for feat in _best_feats :\r\n",
        "    feat = tuple(feat)\r\n",
        "\r\n",
        "    if feat not in freq_dict.keys():\r\n",
        "      freq_dict[feat] = 1\r\n",
        "    else:\r\n",
        "      freq_dict[feat] += 1 \r\n",
        "\r\n",
        "  keys = list(freq_dict.keys()) \r\n",
        "  keys = [ list(key) for key in keys]\r\n",
        "  vals = freq_dict.values()\r\n",
        "  vals = [ val for val in vals ]\r\n",
        "  \r\n",
        "  max_counter_idx = np.argmax(vals)\r\n",
        "  print('counts of same feats :',vals)\r\n",
        "  max_key = list(keys[max_counter_idx])\r\n",
        "\r\n",
        "  for i in range(len(_best_feats) ) :\r\n",
        "    \r\n",
        "    if _best_feats[i] == max_key :\r\n",
        "      losses_best_feat.append(best_loss[i])\r\n",
        "\r\n",
        "      if best_loss[i] < min_key_loss :\r\n",
        "        min_key_loss = best_loss[i]\r\n",
        "        max_key_params =hyper_params[i]\r\n",
        "\r\n",
        "  fig, ax = plt.subplots()\r\n",
        "  fig.set_figheight(10)\r\n",
        "  fig.set_figwidth(40)\r\n",
        "  ax.bar(range(len(vals) ),vals)\r\n",
        "  ax.set_title('features histogram')\r\n",
        "  ax.set_xticks(range(len(vals) ))\r\n",
        "  ax.set_xticklabels(keys,fontsize = 'xx-small')\r\n",
        "  plt.show(ax)\r\n",
        "\r\n",
        "  plt.figure(figsize=(8,8))\r\n",
        "  hist_losses_best_feat = plt.hist(losses_best_feat,bins = round(len(losses_best_feat)/2 ) )\r\n",
        "  plt.title('loss histogram')\r\n",
        "  plt.show(hist_losses_best_feat)\r\n",
        "\r\n",
        "  plt.figure(figsize=(8,8))\r\n",
        "  hist_losses = plt.hist(best_loss,bins = round(len(best_loss)/4 ) )\r\n",
        "  plt.title('all loss histogram')\r\n",
        "  plt.show(hist_losses)\r\n",
        "\r\n",
        "  return max_key ,min_key_loss , max_key_params\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VneUKIqXA2Mt"
      },
      "source": [
        "X_train = []\r\n",
        "Y_train = []\r\n",
        "for i in range(class_no):\r\n",
        "    X_train.extend(X[i])\r\n",
        "    Y_train.extend([i for l in range(len(X[i]))])\r\n",
        "\r\n",
        "X_train = np.array(X_train)\r\n",
        "Y_train = np.array(Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNK84WGQAtsP"
      },
      "source": [
        "feats = ['mfcc', 'del-mfcc','del-del-mfcc', 'Spect', 'rms', 'dSdT', 'dS2dT2','zero_crossing_rate', 'spectral_centroid', 'spectral_bandwidth']\r\n",
        "dims = {\r\n",
        "        'STFT':'image',\r\n",
        "        'Spect':'image',\r\n",
        "        'dSdT':'image',\r\n",
        "        'dS2dT2':'image',\r\n",
        "        'mfcc':'coeffs',\r\n",
        "        'del-mfcc':'coeffs',\r\n",
        "        'del-del-mfcc':'coeffs',\r\n",
        "        'rms':'stats',\r\n",
        "        'zero_crossing_rate':'stats',\r\n",
        "        'spectral_centroid':'stats',\r\n",
        "        'spectral_bandwidth':'stats'\r\n",
        "        }\r\n",
        "\r\n",
        "classes = Y_train.max()+1\r\n",
        "class_labels = np.zeros((classes))\r\n",
        "Y_train2 = []\r\n",
        "\r\n",
        "for y in Y_train:\r\n",
        "    t = np.copy(class_labels)\r\n",
        "    t[y] = 1\r\n",
        "    Y_train2.append(t)\r\n",
        "\r\n",
        "Y_train2 = np.array(Y_train2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XH81mQAAbU4"
      },
      "source": [
        "# finding best of the best\r\n",
        "rounds = 100\r\n",
        "best_feats = []\r\n",
        "hyper_params = []\r\n",
        "best_loss = []\r\n",
        "\r\n",
        "for i in range(rounds):\r\n",
        "\r\n",
        "  best_feats_round, hyper_params_round, best_loss_round = wrapper(X_train,Y_train2,dims,feats)\r\n",
        "\r\n",
        "  best_feats.append(best_feats_round)\r\n",
        "  hyper_params.append(hyper_params_round)\r\n",
        "  best_loss.append(best_loss_round)\r\n",
        "\r\n",
        "  print(best_feats[i])\r\n",
        "  print(hyper_params[i])\r\n",
        "  print(best_loss[i]) \r\n",
        "  print('round:'+str(i) )\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV3dFayL2p2S"
      },
      "source": [
        "best_feats = np.array(best_feats)\r\n",
        "hyper_params = np.array(hyper_params)\r\n",
        "best_loss = np.array(best_loss)\r\n",
        "np.save(feat_space+'best_feats',best_feats )\r\n",
        "np.save(feat_space+'hyper_params',hyper_params )\r\n",
        "np.save(feat_space+'best_loss',best_loss )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH33Z8o9lczq"
      },
      "source": [
        "best_feats = np.load(feat_space+'best_feats.npy', allow_pickle=True)\r\n",
        "hyper_params = np.load(feat_space+'hyper_params.npy', allow_pickle=True)\r\n",
        "best_loss = np.load(feat_space+'best_loss.npy', allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJtICSBIItPt"
      },
      "source": [
        "max_key, min_key_loss, max_key_params =  extract_best_feat(best_feats,hyper_params,best_loss)\r\n",
        "print('most frequent best features: ', max_key)\r\n",
        "print('min loss for best features: ',min_key_loss)\r\n",
        "print('params for best features: ',max_key_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5qqXp_P5H-_"
      },
      "source": [
        "plot_model(m,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoT9bQRIJa4E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZiuRWrImgv8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}